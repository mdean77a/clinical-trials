{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab18acf-f968-4038-a147-bf2d92818cde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pypdf\n",
    "# !pip install yt_dlp\n",
    "# !pip install pydub\n",
    "# !pip install chromadb\n",
    "# !pip install langchain_openai\n",
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba9ba329-c6a2-4498-b152-feff85980cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3417d409-10b3-4cf9-97fa-d6cef237d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import HuggingFacePipeline, HuggingFaceHub\n",
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from openai import OpenAI\n",
    "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ab43-e47e-4d9a-87af-ec23f5d8d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the protocol and cut it up into chunks\n",
    "loader = PyPDFLoader(\"TITRE-protocol.pdf\")\n",
    "pages = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(pages)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624d1ba-3762-4c5a-81af-f51efc021bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the chunks and store in Chromadb\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_function = HuggingFaceBgeEmbeddings(model_name=model_name,\n",
    "                                              encode_kwargs=encode_kwargs)\n",
    "persist_directory = 'docs/chroma/'\n",
    "# if os.path.exists(\"docs/chroma/\") and os.path.isdir(persist_directory):\n",
    "#     shutil.rmtree(\"docs/\")\n",
    "#     print(f\"The directory {persist_directory} has been deleted.\")\n",
    "# else:\n",
    "#     print(f\"The directory {persist_directory} did not exist.\")\n",
    "# persist_directory = 'docs/chroma/'\n",
    "vectordb = Chroma.from_documents(documents=splits,\n",
    "                                 embedding=embedding_function,\n",
    "                                 persist_directory=persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84b2f3cb-dd1a-4d9d-9219-fb134a42e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up my local LLM, which is running on the M3 laptop.\n",
    "llm = OpenAI(base_url=\"http://192.168.1.157:1234/v1/\", api_key=\"not-needed\")\n",
    "# llm = OpenAI(base_url=\"http://192.168.1.157:11434\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca5d0ef-ae3b-487a-9d16-15086a519263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRagData(query:str, k=5):\n",
    "    rag_return = vectordb.similarity_search(query, k=k)\n",
    "    prompt = \"\"\n",
    "    for r in rag_return:\n",
    "        prompt = prompt + r.page_content\n",
    "    prompt = query + \"\\n\" + prompt\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1731836c-b696-4abb-bc09-0a7782250671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askQuestion(query: str, k=5):\n",
    "    prompt = getRagData(query,k)\n",
    "    completion = llm.chat.completions.create(\n",
    "        model=\"local-model\", # this field is currently unused\n",
    "        messages=[  \n",
    "            {\"role\": \"system\", \"content\": \"You are an expert helpful assistant.\"},   \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], \n",
    "        temperature=0.01\n",
    "    )\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "582cf53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def askQuestionOllama(query: str, k=5, model=\"deepseek-r1:8b\"):\n",
    "    prompt = getRagData(query, k)\n",
    "    \n",
    "    # Ollama API endpoint\n",
    "    url = \"http://192.168.1.157:11434/api/generate\"\n",
    "    \n",
    "    # Prepare the payload according to Ollama API\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"system\": \"You are an expert helpful assistant.\",\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0.01\n",
    "    }\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        \n",
    "        # Ollama returns the response in the \"response\" field\n",
    "        if \"response\" in result:\n",
    "            print(result[\"response\"])\n",
    "            return result[\"response\"]\n",
    "        else:\n",
    "            print(f\"Unexpected response structure: {result}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Ollama API call: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0392b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askQuestion(query: str, k=5):\n",
    "    prompt = getRagData(query, k)\n",
    "    try:\n",
    "        completion = llm.chat.completions.create(\n",
    "            model=\"meta-llama-3.1-8b-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.01\n",
    "        )\n",
    "        print(completion.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366058e-b0e3-436d-9011-7f76b6da73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestion(\"Describe the inclusion and exclusion criteria of the study.\",15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869e194-ca5a-4a01-9532-e6aaabea9995",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestion(\"Please write a detailed checklist for starting up and activating a clinical site for the study.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef98a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v http://192.168.1.157:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://192.168.1.157:1234/v1/chat/completions\"\n",
    "payload = {\n",
    "    \"model\": \"meta-llama-3.1-8b-instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.01\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=5)\n",
    "    response.raise_for_status()\n",
    "    print(response.json())\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd802b90-d621-4338-8be8-bae52463f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestionOllama(\"Please write a descriptive summary of the study, including the background, significance, and reasons for doing the study.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1957f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "# Enable debugging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "llm = OpenAI(base_url=\"http://192.168.1.157:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "def getRagData(query: str, k=5):\n",
    "    rag_return = vectordb.similarity_search(query, k=k)  # Assuming vectordb is defined\n",
    "    prompt = \"\"\n",
    "    for r in rag_return:\n",
    "        prompt += r.page_content\n",
    "    prompt = query + \"\\n\" + prompt\n",
    "    return prompt\n",
    "\n",
    "def askQuestion(query: str, k=5):\n",
    "    prompt = getRagData(query, k)\n",
    "    try:\n",
    "        completion = llm.chat.completions.create(\n",
    "            model=\"meta-llama-3.1-8b-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.01\n",
    "        )\n",
    "        print(completion.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "\n",
    "# Run the function\n",
    "askQuestion(\"Describe the inclusion and exclusion criteria of the study.\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56d453-5f57-4f76-9126-41e2ca62a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestion(\"What are the primary and secondary endpoints of the study. What are the main hypotheses?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0732f-e176-4960-b8fd-44137ef32186",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestion(\"What are the risks and benefits of participating in the study?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc5e5c-a5e5-442a-8947-e8f6c599025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestion(\"What are the potential adverse events and how are these defined? \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a29f2-43f0-4fc1-a190-568032ab0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestionOllama(\"List all the visits in the study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f867bc-13dd-4636-bbf5-7437d0858899",
   "metadata": {},
   "outputs": [],
   "source": [
    "askQuestion(\"List all the visits in the study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a9079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
